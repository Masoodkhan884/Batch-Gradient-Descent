{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfce16f2-e299-4dc6-a6ed-280c9441cc7f",
   "metadata": {},
   "source": [
    "<h1>Gradient Descent</h1>\n",
    "<p>\n",
    "    Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models. It works by iteratively adjusting the parameters (weights) of the model in the opposite direction of the gradient of the cost function with respect to the parameters. The goal is to find the parameter values that minimize the cost function, leading to the most accurate model predictions.\n",
    "</p>\n",
    "\n",
    "<h2>Batch Gradient Descent</h2>\n",
    "<p>\n",
    "    Batch Gradient Descent is a specific type of Gradient Descent where the entire dataset is used to compute the gradient of the cost function in each iteration. This method involves the following steps:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        <strong>Step 1: Initialization:</strong> Start with an initial set of weights or parameters, usually chosen randomly.\n",
    "    </li>\n",
    "    <li>\n",
    "        <strong>Step 2: Compute Gradient:</strong> Calculate the gradient of the cost function with respect to each parameter using the entire dataset. The gradient represents the direction and rate at which the cost function increases.\n",
    "    </li>\n",
    "    <li>\n",
    "        <strong>Step 3: Update Parameters:</strong> Update the parameters by moving in the opposite direction of the gradient. The magnitude of the step is controlled by the learning rate, a hyperparameter that determines how quickly or slowly the model learns.\n",
    "    </li>\n",
    "    <li>\n",
    "        <strong>Step 4: Repeat:</strong> Repeat steps 2 and 3 until convergence is reached, i.e., when the change in the cost function becomes negligible, indicating that the model has found the optimal parameters.\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    Batch Gradient Descent is characterized by its stability, as it uses the entire dataset to calculate the gradient, which leads to a smooth and consistent descent towards the minimum of the cost function. However, this also means that each iteration can be computationally expensive and time-consuming, especially for large datasets. Additionally, Batch Gradient Descent requires the entire dataset to fit into memory, which may not be feasible for very large datasets.\n",
    "</p>\n",
    "<p>\n",
    "    Despite its computational cost, Batch Gradient Descent is often preferred in scenarios where the dataset is relatively small, or when a high level of accuracy and stability is required in the model training process.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97029e98-106e-4de4-8464-d2acbe39e73f",
   "metadata": {},
   "source": [
    "# Import laibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9432f83c-2865-4d73-bb75-9e9d47c41b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# loading of dataset\n",
    "from sklearn.datasets import load_diabetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30267d04-ab25-49ce-a7db-d8d31617b205",
   "metadata": {},
   "source": [
    "## Spliting Of Dependent And Independent Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2828b8f-2ceb-43d3-b41f-e7da6cb130e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=load_diabetes(return_X_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde3e28f-8bd7-4147-bc08-b4fefe4d92da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x:  (442, 10)\n",
      "shape of y:  (442,)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of x: \",x.shape)\n",
    "print(\"shape of y: \",y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad3b00-248d-4649-be2f-842ebe1eb573",
   "metadata": {},
   "source": [
    "## Train and Test Split of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1e6610-4f33-4e4e-bcb7-e1bfd8efa754",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c5959-0939-4eef-a8ad-f67c793baf22",
   "metadata": {},
   "source": [
    "# Applying the LinearRegression algorithums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd33aa-20fe-41a9-9c8b-a635f810c017",
   "metadata": {},
   "source": [
    "## Fit data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d664400-a4b8-4ab2-96ec-0e005a43bfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LinearRegression()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981c59b0-d585-4045-a7ab-59ba32bd14c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
      "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238]\n",
      "151.88331005254167\n"
     ]
    }
   ],
   "source": [
    "# coeficient and intercept values of the model \n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca07ca39-4aae-4cbd-8c51-3cafde3d0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction from the model\n",
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba263de-1b89-4509-9f8a-30bfde45d793",
   "metadata": {},
   "source": [
    "## To print the R2 score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1eeab1a-4c9b-4ce7-b9d7-e5ee980a7ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score of this model is :  0.4399338661568969\n"
     ]
    }
   ],
   "source": [
    "print(\"r2 score of this model is : \",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e54bd772-6be0-40d1-bae7-42e5228b39ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126cf76e-bbe0-4f8d-915d-f38f5a6dbf9b",
   "metadata": {},
   "source": [
    "# Applying Batch Gradiant descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850fe07-b5fb-4745-b589-d365e06b1ee0",
   "metadata": {},
   "source": [
    "## Class And Function To Handle The Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a672f41-0e48-4e39-92dc-9c4be721f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor():\n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        self.coef_=None\n",
    "        self.intercept_=None\n",
    "        self.lr=learning_rate\n",
    "        self.epochs=epochs\n",
    "\n",
    "    def fit(self,x_train,y_train):\n",
    "        self.intercept_=0\n",
    "        self.coef_=np.ones(x_train.shape[1])\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            #update the coefficent and intercept\n",
    "            y_hat=np.dot(x_train,self.coef_) +self.intercept_\n",
    "            # print(\"shape of y_hat\",y_hat.shape)\n",
    "            intercept_der=-2 * np.mean(y_train - y_hat)\n",
    "            self.intercept_=self.intercept_ -(self.lr * intercept_der)\n",
    "\n",
    "            \n",
    "            coef_der=-2 * np.dot((y_train - y_hat),x_train)/x_train.shape[0]\n",
    "            self.coef_ = self.coef_  - (self.lr * coef_der)\n",
    "        print(self.intercept_,self.coef_)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        return np.dot(x_test,self.coef_) + self.intercept_\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6237156d-51fc-4650-9337-fc59aa2a8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By increasing and decreasing the learning_rate and epochs we can find the best fit lines according to the data distribution\n",
    "GDR=GDRegressor(epochs=1000,learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f37d34e-7d96-4ede-a20d-50f7e5ed334f",
   "metadata": {},
   "source": [
    "# Fit data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3a06ac2-f6b6-4b08-9d6c-85336099ac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.01351687661833 [  14.38990585 -173.7235727   491.54898524  323.91524824  -39.32648042\n",
      " -116.01061213 -194.04077415  103.38135565  451.63448787   97.57218278]\n"
     ]
    }
   ],
   "source": [
    "GDR.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1a212-f09c-41f0-9012-b972305fc566",
   "metadata": {},
   "source": [
    "# Pridiction from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669f9fdd-b1f5-482b-be97-2615746ce1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([152.26392304, 198.96222354, 127.66111541, 104.59596478,\n",
       "       265.23062371, 252.09467525, 112.76592254, 115.72549839,\n",
       "        96.37765691, 187.64845451, 144.9482918 , 172.110596  ,\n",
       "       178.81497695, 136.51444368, 292.15564227,  87.25795061,\n",
       "       202.18473262, 149.11155912, 132.30895031, 128.70828962,\n",
       "       148.38757935, 171.81318343, 150.93593445, 174.47559507,\n",
       "       127.76388814, 221.82234243, 199.96855698, 101.54518353,\n",
       "        54.85644772, 237.61948938, 244.2801351 , 112.91877003,\n",
       "        68.12192242,  96.00468527, 204.32975531, 163.99882781,\n",
       "       160.95172334, 191.90398957, 113.33794145, 238.46002509,\n",
       "       141.40211434, 120.45598718, 188.12639096, 186.46474321,\n",
       "       174.98259299, 143.24561624, 168.80798895, 299.18508813,\n",
       "       105.40854525, 169.51466009, 254.37509674, 142.60026818,\n",
       "       151.7158263 , 122.70403085, 191.52875115,  94.27792144,\n",
       "       129.03875584,  75.96073902, 157.91752518, 156.36603694,\n",
       "       163.20324594, 160.93274887, 102.3002858 , 227.7604113 ,\n",
       "       146.38764326, 130.09088849, 161.03126295, 192.83334912,\n",
       "       122.94795619, 126.85846035, 217.79918548, 199.2052686 ,\n",
       "       123.42023802, 154.19075213, 146.2669768 , 112.40128412,\n",
       "        80.42448582,  77.96472134, 169.32977609,  81.10892485,\n",
       "        97.26108764,  97.98086796, 179.31830771, 275.58154656,\n",
       "       206.20895128, 146.76562108, 282.5643577 , 202.33755312,\n",
       "        98.43145491])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=GDR.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cd433-f0a3-4c7f-aa14-536d599864ed",
   "metadata": {},
   "source": [
    "# r2_score of GDR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8cc8b12-1611-4e23-bf19-c71c6d92cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score of GDR model is : 0.4534503034722803\n"
     ]
    }
   ],
   "source": [
    "print(\"r2_score of GDR model is :\",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2e3b7-0578-4006-8223-8486fce8ee28",
   "metadata": {},
   "source": [
    "# iT accuracy is improving by appling the GDR instead of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814288bb-c4b6-4b51-9e12-d2a50ca1aff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
